{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess file and load file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains functions for preprocessing files. Given a list of documents' names, first it determines if these documents have already been preprocessed. If one document has not been preprocessed, the function then will process this document, and then store this document in hard disc. If it has been preprocessed, this function will directly load its preprocessed version. Since preprocessing cost highly, this mechanism saves lots of time.\n",
    "\n",
    "Here we are using spacy library and its German package to do the preprocessing. \n",
    "\n",
    "* **file_utils.py** contains code for creating one json from list of json strings. This json file is then loaded by **json.load**.\n",
    "* **configuration.py** defines all path prefixes for load and store documents.\n",
    "* The first part of this file is for loading a non german documents list. Since our program only processes German documents, these non German documents will have an negative impact for preprocessing and should be removed first.\n",
    "* Function **\\_read\\_content\\_of\\_paragraphs** reads documents in json, one paragraph by one paragraph.\n",
    "* Function **read_page_and_par_Info** reads a document, and return the page number of each paragraph of this document.\n",
    "* Function **\\_preprocess\\_paragraphs** is to preprocess data, remove useless words, and then to store these preprocessed, clean documents to hard disc. \n",
    "* Function **\\_load\\_preprocessed\\_paragraphs** is to load preprocessed, clean document from hard disc.\n",
    "* Function **get_clean_documents** is the interface for other users. They should only use this function to get clean, preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "# In src/configuration.py, we define these paths:\n",
    "#     FILE_PATH : where the code get raw json documents.\n",
    "#     CLEAN_FILE_PREFIX : used to distinguish clean, preprocessed documents from raw documents.\n",
    "#     NON_GERMAN_FILE_PATH : where the code get non German file list  \n",
    "#     SHARE_SPACE_FOLDER_PATH : where the code store clean, preprocessed documents. It can be accessed by all group members. \n",
    "############################################################################################################################\n",
    "%run src/file_utils.py\n",
    "%run src/configuration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non German file list\n",
    "# Here we load this list from a txt file.\n",
    "# This list will be used to filter all non german files in function get_clean_documents.\n",
    "with open(NON_GERMAN_FILE_PATH) as f:    \n",
    "    non_german_documents = [line.strip() + '.json' for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw json file\n",
    "def _read_content_of_paragraphs(file_name):\n",
    "    contents = []\n",
    "    try:\n",
    "        document_parts = json.loads(FileUtils.fix_json(file_name))\n",
    "        for part in document_parts:\n",
    "            if part['type'] == 'paragraph':\n",
    "                contents.append(part['content'])\n",
    "    except:\n",
    "        print('Bad file: ' + file_name)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "# Given a document, this function returns each paragraph's corresponding page number in this document.\n",
    "# parameter :\n",
    "#          file_name: the absolute path of a document.\n",
    "# return :\n",
    "#          contents: a list, in which each entry is an object which contains paragraph number and this \n",
    "#          paragraph's corresponding page number.\n",
    "###########################################################################################################################\n",
    "def read_page_and_par_Info(file_name):\n",
    "    contents = []\n",
    "   \n",
    "    try:\n",
    "        data = json.loads(FileUtils.fix_json(file_name))\n",
    "        for item in data:\n",
    "            typeDoc = item['type']\n",
    "            if typeDoc == 'paragraph':\n",
    "                contents.append({\n",
    "                    'page':item['pagenumber'],\n",
    "                    'paragraph':item['counter']\n",
    "                })\n",
    "    except:\n",
    "        pass\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries contains as a \"key\" values, which we want to replace and\n",
    "# as \"value\" the desired values.\n",
    "replace_dict = dict()\n",
    "replace_dict['gCO2'] = 'CO2'\n",
    "\n",
    "# meaningful combinations of letters and digits, that we want to preserve.\n",
    "digit_and_letter_combintaions = list()\n",
    "digit_and_letter_combintaions.append('CO2')\n",
    "\n",
    "# Here we use spacy to do preprocessing, remove stop words, useless words and digits. \n",
    "# After preprocessing, we get clean documents.\n",
    "# Then we store these clean doucments in hard disc.\n",
    "# note: here input document should contain its absolute path.\n",
    "def _preprocess_paragraphs(document):\n",
    "    nlp = spacy.load(\"de\",disable=['parser', 'ner'])\n",
    "    paragraphs = _read_content_of_paragraphs(document)\n",
    "    # here is try to remove company name in documents, becasue these name is useless and harmful for our approach\n",
    "    company_name = document[len(FILE_PATH):document.find('-')].lower()\n",
    "    lemmatized_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        # remove the - in document\n",
    "        content_of_document = paragraph.replace('-\\n','')\n",
    "        content_of_document = content_of_document.replace('\\n',' ')\n",
    "    \n",
    "        #replace all entries, which can be found in replacement dictionary\n",
    "        for replace_source, replace_target in replace_dict.items():\n",
    "            content_of_document = content_of_document.replace(replace_source, \n",
    "                                                              replace_target)\n",
    "    \n",
    "        #remove the character we don't need\n",
    "        remove_char = content_of_document.maketrans('-',' ','+*<>%/&$')\n",
    "        content_of_document = content_of_document.translate(remove_char)\n",
    "    \n",
    "        words = nlp(content_of_document)\n",
    "        filtered_words = [word for word in words if word.lower_ not in STOP_WORDS] \n",
    "        filtered_words = [word for word in filtered_words if word.pos_ != 'NUM' and word.pos_ != 'SYM' and word.pos_ != 'PUNCT']\n",
    "        filtered_words = [word for word in filtered_words if not word.is_digit]\n",
    "        filtered_lemmata = [word.lemma_ for word in filtered_words]\n",
    "    \n",
    "        final = []  \n",
    "        for lemma in filtered_lemmata:\n",
    "            #remove the lemma contain company names\n",
    "            if company_name in lemma.lower():\n",
    "                continue\n",
    "                \n",
    "            if(any(c.isdigit() for c in lemma)):\n",
    "                for combination in digit_and_letter_combintaions: \n",
    "                    if combination in lemma:\n",
    "                        final.append(lemma)\n",
    "            else:\n",
    "                #remove the words contain dot\n",
    "                if '.' not in lemma:\n",
    "                    final.append(lemma)\n",
    "        \n",
    "        lemmatized_content = \" \".join(final)\n",
    "        lemmatized_paragraphs.append(lemmatized_content.lower())\n",
    "        \n",
    "    #output the result into file \n",
    "    filename = CLEAN_FILE_PREFIX  + document[len(FILE_PATH):] # remove file path, only preserve the document's name\n",
    "    filename = SHARE_SPACE_FOLDER_PATH + filename  # add the clean data folder path\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(lemmatized_paragraphs, outfile) # save clean data in json file, which is easy to read\n",
    "    \n",
    "    return lemmatized_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function load clean paragraphs which we already have preprocessed and saved.\n",
    "# Given an absolute path of document, it returns a list, in which each entry is a paragraph of this document.\n",
    "def _load_preprocessed_paragraphs(document):\n",
    "    with open(document, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "    return datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################\n",
    "# Given a list of documents' name( without path), this function does preprocessing to these documents, and returns \n",
    "# preprocessed, clean documents. The form of return value depends on parameter, get_paragraph.\n",
    "# parameter :\n",
    "#     documents_list: a list, contain string of file name, which we want to preprocessing\n",
    "#     get_paragraph: True if you want to get every paragrah, False if you want to get the whole document\n",
    "#     logging: default = False. If set it as true, then it will print information about which document is currently \n",
    "#     preprocessing and which document has been already done and which document is non German document.\n",
    "# return :\n",
    "#     documents_clean: This returns value depends on the value of the parameter, get_paragraph. \n",
    "#                      if get_paragraph = false, it returns a list contains several strings, and each string contains a whole \n",
    "#                      document.\n",
    "#                      if get_paragraph = true, it returns a list contains several lists of strings, and here each string is a\n",
    "#                      paragraph of one document. All strings in one list make up a document.\n",
    "#     documents_clean_name: a list contains string, every output documents' corresponding name\n",
    "#################################################################################################################################\n",
    "\n",
    "def get_clean_documents(documents_list, get_paragraph = False, logging=False):\n",
    "    documents_clean = list()\n",
    "    documents_clean_name = list()\n",
    "\n",
    "    for document in documents_list:\n",
    "        # fist check if this document is english\n",
    "        if document in non_german_documents:\n",
    "            if logging:\n",
    "                print(\"this file \"+ document + ' is non german, skip it')\n",
    "            continue \n",
    "        # second check if this doc already be preprocessed\n",
    "        my_file = Path(SHARE_SPACE_FOLDER_PATH + CLEAN_FILE_PREFIX + document)\n",
    "        if my_file.is_file():\n",
    "            # if already exist, we directly load the clean data from hard disk\n",
    "            if logging: \n",
    "                print(CLEAN_FILE_PREFIX + document + \" has already done preprocess\")\n",
    "            # load file!\n",
    "            documents_clean_name.append(document)\n",
    "            documents_clean.append(_load_preprocessed_paragraphs(SHARE_SPACE_FOLDER_PATH + CLEAN_FILE_PREFIX + document))\n",
    "        else:\n",
    "        # if not find, we do preprocess for this document, and save it in hard disk\n",
    "            documents_clean_name.append(document)\n",
    "            documents_clean.append(_preprocess_paragraphs(FILE_PATH + document))\n",
    "\n",
    "    if not get_paragraph:\n",
    "        # if we don't want paragraph, but the whole report. Here we join all paragraph to get a whole report. \n",
    "        documents_tmp = list()\n",
    "        for document in documents_clean:\n",
    "            document_tmp = \" \".join(para for para in document)\n",
    "            documents_tmp.append(document_tmp)\n",
    "        documents_clean = documents_tmp\n",
    "    return documents_clean, documents_clean_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
