{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess file and load file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains funtions for preprocessing files. Given list of documents' names, it can at first determine if the document have already been preprocessed. If this document has not been preprocessed, the funciton at first preprocessed this document, and then store this document in hard disc. If it has been preprocessed, this function directly load its preprocessed vision. Since preprocessing cost highly, this mechanism save lots of time.\n",
    "\n",
    "Here we are using spacy library and its German package to do the preprocessing.\n",
    "\n",
    "* **file_utils.py** contains code for reading json file.\n",
    "* **configuration.py** contains original and after preprocessing documents.\n",
    "* The first part of this file is code for loading non german documents list. Since our program only process German documents, these non german documents will have an inverse impact for preprocessing and should be removed at first.\n",
    "* function ** readContentOfParagraphs** read documents in json, one paragraph by one paragraph\n",
    "* funciton ** lemmatize_paragraphs** is to clean data.\n",
    "* function ** load_lemmatization_paragraph** is to load preprocessed document from hard disc.\n",
    "* function ** get_clean_data** is the interface for other users. They should only use this function to get clean, preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py   #store useful file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TYPE = 'type'\n",
    "PARAGRAPH = 'paragraph'\n",
    "CONTENT = 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read non german file list\n",
    "# we have a Non_german_file list file, so here we just read this list from that txt file.\n",
    "# this list will be used to filter all non german files\n",
    "with open(NON_GERMAN_FILE_PATH) as f:\n",
    "    non_german_documents = f.readlines()\n",
    "non_german_documents = [x.strip() for x in non_german_documents] \n",
    "temp_docu = list()\n",
    "for document in non_german_documents:\n",
    "    temp_docu.append(document + '.json')\n",
    "non_german_documents = temp_docu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funciton to read raw json file\n",
    "def readContentOfParagraphs(file_name):\n",
    "    contents = []\n",
    "    try:\n",
    "        data = json.loads(FileUtils.fix_json(file_name))\n",
    "        for item in data:\n",
    "            typeDoc = item[TYPE]\n",
    "            if typeDoc == PARAGRAPH:\n",
    "                contents.append(item[CONTENT])\n",
    "    except:\n",
    "        print('Bad file: ' + file_name)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace_dict = dict()\n",
    "replace_dict['gCO2'] = 'CO2'\n",
    "\n",
    "# meaningful combinations of letters and digits, that we want to preserve.\n",
    "digit_and_letter_combintaions = list()\n",
    "digit_and_letter_combintaions.append('CO2')\n",
    "\n",
    "# here we using spacy to do preprocessing, remove stop words, useless words and so on\n",
    "def lemmatize_paragraphs(document):\n",
    "    nlp = spacy.load(\"de\",disable=['parser', 'ner'])\n",
    "    paragraphs = readContentOfParagraphs(document)\n",
    "    # here is try to remove company name in documents, becasue these name is useless and harmful for our approach\n",
    "    company_name = document[len(FILE_PATH):document.find('-')].lower()\n",
    "    lemmatized_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        # remove the - in document\n",
    "        content_of_document = paragraph.replace('-\\n','')\n",
    "        content_of_document = content_of_document.replace('\\n',' ')\n",
    "    \n",
    "        #replace all entries, which can be found in replacement dictionary\n",
    "        for replace_source, replace_target in replace_dict.items():\n",
    "            content_of_document = content_of_document.replace(replace_source, \n",
    "                                                              replace_target)\n",
    "    \n",
    "        #remove the character we don't need\n",
    "        remove_char = content_of_document.maketrans('-',' ','+*<>%/&$')\n",
    "        content_of_document = content_of_document.translate(remove_char)\n",
    "    \n",
    "        sentence = nlp(content_of_document)\n",
    "        filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS] \n",
    "        filtered_words_withoutnum = [word for word in filtered_words if word.pos_ != 'NUM']\n",
    "        filtered_words_withoutsym = [word for word in filtered_words_withoutnum if word.pos_ != 'SYM']\n",
    "        filtered_words_withoutdigits = [word for word in filtered_words_withoutsym if not word.is_digit]\n",
    "        filtered_words_withoutpunc = [word for word in filtered_words_withoutdigits if word.pos_ != 'PUNCT']\n",
    "        filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "    \n",
    "        final = []  \n",
    "        for item in filtered_lemmas:\n",
    "            #remove the words contain digit except of co2\n",
    "            if company_name in item.lower():\n",
    "                continue\n",
    "                \n",
    "            if(any(c.isdigit() for c in item)):\n",
    "                for combination in digit_and_letter_combintaions: \n",
    "                    if combination in item:\n",
    "                        final.append(item)\n",
    "            else:\n",
    "                #remove the words contain dot\n",
    "                if '.' not in item:\n",
    "                    final.append(item)\n",
    "        \n",
    "        lemmatized_content = \" \".join(item for item in final)\n",
    "        lemmatized_paragraphs.append(lemmatized_content.lower())\n",
    "        \n",
    "    #output the result into file \n",
    "    if document.startswith(FILE_PATH):# if file path is in document name, remove it\n",
    "        filename = CLEAN_FILE_PREFIX  + document[len(FILE_PATH):]\n",
    "    filename = SHARE_SPACE_FOLDER_PATH + filename  # add the clean data folder path\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(lemmatized_paragraphs, outfile) # save clean data in json file, which is easy to read\n",
    "        \n",
    "    return lemmatized_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function try to load clean paragraph which we already have done preprocess and saved in folder\n",
    "def load_lemmatization_paragraph(document):\n",
    "    with open(document, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "    return datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter :\n",
    "#     documents_list: a list, contain string of file name, which we want to preprocessing\n",
    "#     get_paragraph: True if you want to get every paragrah, False if you want to get the whole document\n",
    "#     logging: default = False, if set it as true, then it will print information about which document are currently preprocessing\n",
    "#     and which document has been already done.\n",
    "# return :\n",
    "#     documents_clean: a list contain string, which contain the whole document\n",
    "#     documents_clean_name: a list contain string, every output documents' corresponding name\n",
    "\n",
    "def get_clean_data(documents_list, get_paragraph = False, logging=False):\n",
    "    documents_clean = list()\n",
    "    documents_clean_name = list()\n",
    "\n",
    "    for document in documents_list:\n",
    "        # fist check if this document is english\n",
    "        if document in non_german_documents:\n",
    "            if logging:\n",
    "                print(\"this file \"+ document + ' is non german, skip it')\n",
    "            continue \n",
    "        # second check if this doc already be preprocessed\n",
    "        my_file = Path(SHARE_SPACE_FOLDER_PATH + CLEAN_FILE_PREFIX + document)\n",
    "        if my_file.is_file():\n",
    "        # if already exist, we directly load the clean data from hard disk\n",
    "            if logging: \n",
    "                print(CLEAN_FILE_PREFIX + document + \" has already done preprocess\")\n",
    "            # load file!\n",
    "            documents_clean_name.append(document)\n",
    "            documents_clean.append(load_lemmatization_paragraph(SHARE_SPACE_FOLDER_PATH + CLEAN_FILE_PREFIX + document))\n",
    "        else:\n",
    "        # if not find, we do preprocess for this document, and save it in hard disk\n",
    "            documents_clean_name.append(document)\n",
    "            documents_clean.append(lemmatize_paragraphs(FILE_PATH + document))\n",
    "\n",
    "    if not get_paragraph:\n",
    "        # if we don't want paragraph, but the whole report. Here we join all paragraph to get a whole report. \n",
    "        documents_tmp = list()\n",
    "        for document in documents_clean:\n",
    "            document_tmp = \" \".join(para for para in document)\n",
    "            documents_tmp.append(document_tmp)\n",
    "        documents_clean = documents_tmp\n",
    "    return documents_clean, documents_clean_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}