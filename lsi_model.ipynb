{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlab_LSI_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains an implementation of an information query method, based on Latent Semantic Indexing (LSI) technique. LSI is a mature and wildely used technique in nature language processing. It use a rank-reduced Singular Value Decomposition( SVD) on TF-IDF matrix ( introduction of TF-IDF matrix is in file TFIDF_company_chart). LSI considers words which occur in the same document tends to have relationship, and this rank-reduced  SVD can merge these words with similar meaning or strong relationship. One advantage of information query method based on LSI is that it can search highly related contents, even though the words in query may not appear in these contents. Following are some Mathematic details.\n",
    "\n",
    "\n",
    "The procedure is:\n",
    "\n",
    "   1. We at first load 1000 clean documents, and use function TfidfVectorizer from sklearn libary to build their TF-IDF matrix.  \n",
    "\n",
    "   2. Given the query string, we look it as a new document, and also get its TF-IDF vector( since only one document) according to the TF-IDF matrix we get in step 1. Here we assume all input queries do not contain repeat words. Therefore for every words in the query, we simply find its correspondent index in TF-IDF matrix, and assign its value as 1 * IDF(index). And for other words which do not in the query, we simply assign them as 0.  \n",
    "\n",
    "   3. Apply rank reduced SVD at TF-IDF matrix, we get:\n",
    "    <img src=\"https://latex.codecogs.com/svg.latex?A&space;\\approx&space;A_{k}&space;=&space;U_{K}S_{K}V_k^T\" title=\"A \\approx A_{k} = U_{K}S_{K}V_k^T\" />\n",
    "    Here this A is original TF-IDF matrix, and Ak is its K dimension approximation. We can simply get this Ak by evaluating the formula above. Here we can get Sk, Tk and Dk by letting the singluar value matrix S only preserve k max singluar value( Sk), and let left singluar vector matrix U, right singluar vector matrix V only preserve their first k row vectors and k col vectors, respectively.\n",
    "    In the code, we simply assign k = 100, and use scipy's sparse SVDs function to evaluate this formula efficiently. After this step, we transfer this TF-IDF matrix to low dimension space.  \n",
    "    \n",
    "   4. To compute similarity of query and document vectors in low dimension space, we should also transfer the query vector from step 2 to this low dimension space. We do that by evaluating this formula:\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?q_{k}&space;=&space;S^{-1}_{k}U^T_dq\" title=\"q_{k} = S^{-1}_{k}U^T_dq\" />\n",
    "    Here qk is query's k dimension vector. And q is its original vector. This works because exactly every col vector in right singluar matrix V describle one document, and in LSI technique we also see the query as a new document.  \n",
    "    \n",
    "   5. Finally, we use cosine similarities to compute the similarities between the query vector and documents vectors. And take the top 10 similar documents as result. The cosine similarities formular is:\n",
    "   <img src=\"https://latex.codecogs.com/svg.latex?sim(q,d)&space;=&space;\\frac{q*d}{\\begin{vmatrix}&space;q&space;\\end{vmatrix}\\begin{vmatrix}&space;d&space;\\end{vmatrix}}\" title=\"sim(q,d) = \\frac{q*d}{\\begin{vmatrix} q \\end{vmatrix}\\begin{vmatrix} d \\end{vmatrix}}\" />\n",
    "   This cosin similarities only consider how two vectors' direction are similar. This is useful, since usually the scala value of query vector is smaller than the document vector because here query does not contain any repeat word.\n",
    "   \n",
    "In code below, I use \"Sportbekleidung schuh\" as a query, and the top 10 similar (or related) documents I got from this program are 8 PUMA and 1 Addidas and 1 Zalando, which all sell sportswear and shoes. This result shows that this program works. The keypoint here is that I actually do not know if these 2 words occur in these top 10 documents, but LSI technique can still show me these highly related result. This is impossible with traditional key word search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds, eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py\n",
    "%run 'load_and_prepro_document.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI\n",
    "LSI is actually just doing SVD at TF-IDF matrix, and to get an approximate TF-IDF matrix with low number of dimension.\n",
    "Here I try to use LSI to realize a information retrieval application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here just use os lib to get the 1000 documents in this folder\n",
    "import os\n",
    "documents_list = list()\n",
    "for root, dirs, files in os.walk(\"./LabShare/data/all/json\", topdown=False):\n",
    "    for name in files:\n",
    "        documents_list.append(name)\n",
    "documents_list = documents_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5903236865997314\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# here I override the preProcess() in fit_transform(). Because the input data is already preprocessed.\n",
    "def preProcess(s):\n",
    "    return s\n",
    "my_doc, my_doc_name = get_clean_data(documents_list)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform( my_doc )\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search query is: \n",
      "{'sportbekleidung', 'schuh'}\n",
      "------------------------------------\n",
      "related document: \t related score\n",
      "PUMA-QuarterlyReport-2012-Q3.json:\t0.9705766715598944\n",
      "PUMA-QuarterlyReport-2012-Q2.json:\t0.9584773368670201\n",
      "PUMA-QuarterlyReport-2015-Q1.json:\t0.9543901309787146\n",
      "PUMA-QuarterlyReport-2014-Q3.json:\t0.9507635544600765\n",
      "PUMA-QuarterlyReport-2010-Q2.json:\t0.9435183542968074\n",
      "PUMA-QuarterlyReport-2011-Q3.json:\t0.9304272544834216\n",
      "PUMA-QuarterlyReport-2010-Q1.json:\t0.9128649922630564\n",
      "PUMA-AnnualReport-2013.json:\t0.8812087556023984\n",
      "Adidas-AnnualReport-2016.json:\t0.3034457396028766\n",
      "Zalando-AnnualReport-2015.json:\t0.21861967178319974\n"
     ]
    }
   ],
   "source": [
    "# now compute the input query's vector.\n",
    "query = 'Sportbekleidung schuh '   #query string\n",
    "\n",
    "# step 1, do same preprosseing for this query\n",
    "nlp = spacy.load(\"de\")\n",
    "sentence = nlp(query, disable=['parser', 'ner'])\n",
    "filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "filtered_words_withoutpunc = [word for word in filtered_words_withoutdigits if word.pos_ != 'PUNCT']\n",
    "filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "\n",
    "vocabularly = set()\n",
    "for word in filtered_lemmas:\n",
    "    vocabularly.add(word.replace('\\n', '').strip().lower())\n",
    "\n",
    "new_vocab = set()\n",
    "for u in vocabularly:\n",
    "    if u != '':\n",
    "        new_vocab.add(u)\n",
    "\n",
    "# step 2, generate query's tf-idf vector\n",
    "query_vector_ori = np.zeros(tfidf_matrix.shape[1]) #initilize the query vector\n",
    "idf = vectorizer.idf_\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "# find my words in this feature_name list, and its corresponding index\n",
    "print(\"search query is: \")\n",
    "print(new_vocab)\n",
    "for words in new_vocab:\n",
    "    idx = feature_name.index(words)\n",
    "    query_vector_ori[idx] = idf[idx]\n",
    "# do normalize\n",
    "query_vector_ori = query_vector_ori/np.linalg.norm(query_vector_ori)\n",
    "\n",
    "# step3, transfer the origin vector to low_dim space\n",
    "k = 100\n",
    "u, s, vt = svds(tfidf_matrix.T, k=k)  # transpose the tfidf_matrix, get item*document\n",
    "#here k is the remaining dimension. could from 1 to (number of document-1)\n",
    "# d_hat = s.inv*U.t*d    \n",
    "s_dig = np.diag(s)\n",
    "query_vector_low_dim = ((np.linalg.inv(s_dig)).dot(u.T)).dot(query_vector_ori)\n",
    "# get query in low dim\n",
    "\n",
    "# step4, compute the similarity\n",
    "def calculate_simility(q1,q2):\n",
    "    sim = q1.dot(q2)/(np.linalg.norm(q1)*np.linalg.norm(q2))\n",
    "    return sim\n",
    "sim = np.zeros(vt.shape[1])\n",
    "for i in range(0,vt.shape[1]):\n",
    "    sim[i] = calculate_simility(query_vector_low_dim,vt[:,i])\n",
    "\n",
    "# step5, take top 10 similar document\n",
    "top_idx = np.argsort(-sim)[0:10]  # here -sim, since I want t get decending order sort,and get the top 3 index\n",
    "print('------------------------------------')\n",
    "print('related document: \\t related score')\n",
    "for i in top_idx:\n",
    "    print(my_doc_name[i]+':\\t'+ str(sim[i]))\n",
    "\n",
    "# try to find some way to connect document and this index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
