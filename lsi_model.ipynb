{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlab_LSI_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains an implementation of an information query method, based on Latent Semantic Indexing (LSI) technique. LSI is a mature and widely used technique in natural language processing. It uses a rank-reduced Singular Value Decomposition( SVD) on TF-IDF matrix ( introduction of TF-IDF matrix is in file TFIDF_company_chart). LSI considers words which occur in the same document tends to have a relationship, and this rank-reduced  SVD can merge these words with similar meaning or strong relationship. One advantage of information query method based on LSI is that it can search highly related contents, even though the words in the query may not appear in these contents. Following are some Mathematics details.\n",
    "\n",
    "\n",
    "The procedure is:\n",
    "\n",
    "   1. We at first load 1000 clean documents, and use function TfidfVectorizer from sklearn library to build their TF-IDF matrix.  \n",
    "\n",
    "   2. Given the query string, we look it as a new document and also get its TF-IDF vector( since only one document) according to the TF-IDF matrix we get in step 1. Here we assume all input queries do not contain repeat words. Therefore for every word in the query, we simply find its correspondent index in TF-IDF matrix and assign its value as $1 * IDF(index)$. And for other words which do not in the query, we simply assign them as $0$.  \n",
    "\n",
    "   3. Apply rank reduced SVD at TF-IDF matrix, and we get:\n",
    "$$A \\approx A_{k} = U_{k}S_{k}V_k^T$$\n",
    "     \n",
    "    Here this $A$ is the original TF-IDF matrix, and $A_{k}$ is its $k$ dimension approximation. We can simply get this $A_{k}$ by evaluating the formula above. Here we can get $S_{k}$, $U_{k}$ and $V_{k}$ by letting the singular value matrix $S$ only preserve $k$ max singular value( $S_{k}$), and let left singular vector matrix $U$, right singular vector matrix $V$ only preserve their first $k$ row vectors and $k$ col vectors, respectively.\n",
    "    In the code, we simply assign $k = 100$ and use scipy's sparse SVDs function to evaluate this formula efficiently. After this step, we actually approximate this TF-IDF matrix in a low dimension space.  \n",
    "    \n",
    "   4. To compute the similarity of query and document vectors in low dimension space, we should also transfer the query vector from step 2 to this low dimension space. We do that by evaluating this formula:\n",
    "$$q_{k} = S^{-1}_{k}U^T_kq$$\n",
    "\n",
    "    Here $q_{k}$ is query's $k$ dimension approximation. And $q$ is its original vector. This works because exactly every col vector in the right singular matrix $V$ describe one document, and in LSI technique we also see the query as a new document.  \n",
    "    \n",
    "   5. Finally, we use cosine similarities to compute the similarities between the query vector and documents vectors. And take the top 10 similar documents as result. The cosine similarities formula is:\n",
    "$$sim(q,d) = \\frac{q*d}{\\begin{vmatrix} q \\end{vmatrix}\\begin{vmatrix} d \\end{vmatrix}}$$\n",
    "   \n",
    "    This cosine similarities only consider how the two vectors' directions are similar. This is useful since usually the scala value of the query vector is different from the document vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import spacy\n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "from spacy.lang.de.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py\n",
    "%run 'load_and_prepro_document.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect 1000 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here just use os lib to get the 1000 documents in this folder\n",
    "import os\n",
    "documents_list = list()\n",
    "for root, dirs, files in os.walk(FILE_PATH, topdown=False):\n",
    "    for name in files:\n",
    "        documents_list.append(name)\n",
    "documents_list = documents_list[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess documents, and build TF-IDF matrix based on these documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.361062288284302\n",
      "The number of input documents is 950, the shape of tfidf matrix is (950, 233447)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# here I override the preProcess() in fit_transform(). Because the input data is already preprocessed.\n",
    "def preProcess(s):\n",
    "    return s\n",
    "my_doc, my_doc_name = get_clean_data(documents_list)\n",
    "vectorizer = TfidfVectorizer(preprocessor=preProcess)\n",
    "tfidf_matrix = vectorizer.fit_transform( my_doc )\n",
    "print (time.time() - start_time)\n",
    "print (f\"The number of input documents is {len(my_doc)}, the shape of tfidf matrix is {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Sportbekleidung schuh '   #query string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, do same preprosseing for this query\n",
    "nlp = spacy.load(\"de\")\n",
    "sentence = nlp(query, disable=['parser', 'ner'])\n",
    "filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "filtered_words_withoutpunc = [word for word in filtered_words_withoutdigits if word.pos_ != 'PUNCT']\n",
    "filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "\n",
    "vocabularly = set()\n",
    "for word in filtered_lemmas:\n",
    "    vocabularly.add(word.replace('\\n', '').strip().lower())\n",
    "\n",
    "new_vocab = set()\n",
    "for u in vocabularly:\n",
    "    if u != '':\n",
    "        new_vocab.add(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate query's TF-IDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search query is: \n",
      "{'sportbekleidung', 'schuh'}\n"
     ]
    }
   ],
   "source": [
    "# step 2, generate query's tf-idf vector\n",
    "query_vector_ori = np.zeros(tfidf_matrix.shape[1]) #initilize the query vector\n",
    "idf = vectorizer.idf_\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "# find my words in this feature_name list, and its corresponding index\n",
    "print(\"search query is: \")\n",
    "print(new_vocab)\n",
    "for words in new_vocab:\n",
    "    idx = feature_name.index(words)\n",
    "    query_vector_ori[idx] = idf[idx]\n",
    "# do normalize\n",
    "query_vector_ori = query_vector_ori/np.linalg.norm(query_vector_ori)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute low dimension approximation of the TF-IDF matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3, transfer the origin tf-idf matrix to low_dim space\n",
    "k = 100\n",
    "u, s, vt = svds(tfidf_matrix.T, k=k)  # transpose the tfidf_matrix, get item*document\n",
    "# here k is the remaining dimension. could from 1 to (number of document-1)\n",
    "# d_hat = s.inv*U.t*d    \n",
    "s_dig = np.diag(s)\n",
    "query_vector_low_dim = ((np.linalg.inv(s_dig)).dot(u.T)).dot(query_vector_ori)\n",
    "# get query in low dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4, compute the similarity\n",
    "def calculate_similarity(q1,q2):\n",
    "    sim = q1.dot(q2)/(np.linalg.norm(q1)*np.linalg.norm(q2))\n",
    "    return sim\n",
    "\n",
    "sim = np.zeros(vt.shape[1])\n",
    "for i in range(0,vt.shape[1]):\n",
    "    sim[i] = calculate_simility(query_vector_low_dim,vt[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show top 10 documents which are similar to the input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "related document: \t related score\n",
      "PUMA-QuarterlyReport-2012-Q3.json:\t0.9705994734434994\n",
      "PUMA-QuarterlyReport-2012-Q2.json:\t0.9585080602136418\n",
      "PUMA-QuarterlyReport-2015-Q1.json:\t0.9544564275740416\n",
      "PUMA-QuarterlyReport-2014-Q3.json:\t0.9507846869303268\n",
      "PUMA-QuarterlyReport-2010-Q2.json:\t0.943183614004445\n",
      "PUMA-QuarterlyReport-2011-Q3.json:\t0.9305251076514407\n",
      "PUMA-QuarterlyReport-2010-Q1.json:\t0.9128798788337482\n",
      "PUMA-AnnualReport-2013.json:\t0.8811760861900734\n",
      "Adidas-AnnualReport-2016.json:\t0.30320260536883675\n",
      "Zalando-AnnualReport-2015.json:\t0.21879978741533074\n"
     ]
    }
   ],
   "source": [
    "# step5, take top 10 similar document\n",
    "top_idx = np.argsort(-sim)[0:10]  # here -sim, since I want t get decending order sort,and get the top 3 index\n",
    "print('------------------------------------')\n",
    "print('related document: \\t related score')\n",
    "for i in top_idx:\n",
    "    print(my_doc_name[i]+':\\t'+ str(sim[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 similar (or related) documents I got from this program are 8 PUMA and 1 Addidas and 1 Zalando, which all sell sportswear and shoes. This result shows that this program works. The keypoint here is that I actually do not know if these 2 words occur in these top 10 documents, but LSI technique can still show me these highly related result. This is impossible with traditional key word search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
