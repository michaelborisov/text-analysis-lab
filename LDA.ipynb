{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA - Latent Dirichlet Allocation.\n",
    "\n",
    "### LDA. Quick facts:\n",
    "- Well tested and well known approach.\n",
    "- Automatically extract topics from documents.\n",
    "- Can be directly applied to a new document.\n",
    "\n",
    "### LDA. The way it works:\n",
    "- Each document is seen as a mixture of various topics.\n",
    "- Each word in the document is assigned to the topics with the certain probabilities\n",
    "- Each topic is seen as a set of words with assigned probabilities.\n",
    "- Person should assign topic-names himself, but only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import groupby\n",
    "\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py\n",
    "%run 'load_and_prepro_document.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we will show two use cases of LDA:\n",
    "1. Find the distribution of topic \"Risk Management\" in one document of a given bank throughout paragraphs of this document. [1]\n",
    "2. Show the fluctuation of topic \"Risk Management\" for different banks throughout several years. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect names of all reports, related to banks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "banks = [f for f in listdir(FILE_PATH) if isfile(join(FILE_PATH, f)) and 'bank' in f.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load clean documents, related to banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load paragraphs of 176 documents took 0.38 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lemm_docs_prep, names = get_clean_data(banks,get_paragraph=True)\n",
    "\n",
    "print ('Time to load paragraphs of {0:d} documents took {1:.2f} seconds'.format(\n",
    "        len(names), \n",
    "        time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readPageAndParInfo(file_name):\n",
    "    contents = []\n",
    "   \n",
    "    try:\n",
    "        data = json.loads(FileUtils.fix_json(file_name))\n",
    "        for item in data:\n",
    "            typeDoc = item[TYPE]\n",
    "            if typeDoc == PARAGRAPH:\n",
    "                contents.append({\n",
    "                    'page':item['pagenumber'],\n",
    "                    'paragraph':item['counter']\n",
    "                })\n",
    "    except:\n",
    "        pass\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of lists of paragraphs -> list of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lem_pars = []\n",
    "for sublist in lemm_docs_prep:\n",
    "    for item in sublist:\n",
    "        lem_pars.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us start with the first use case [1]:\n",
    "- Train LDA model by using 127 bank documents. Get 9 topics.\n",
    "- Input a new document.\n",
    "- For each paragraph in this document, calculate its probability of belonging to each topic. \n",
    "- Specify one topic, output most related paragraphs and group  them to pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit and transofrm of CountVectoriser on 91042 paragrpaphs took 2.54 seconds\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "start_time = time.time()\n",
    "tf = tf_vectorizer.fit_transform(lem_pars)\n",
    "\n",
    "print ('Fit and transofrm of CountVectoriser on {0:d} paragrpaphs took {1:.2f} seconds'.format(\n",
    "        len(lem_pars), \n",
    "        time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit phase of LDA took 252.48 seconds\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=9,\n",
    "                                learning_method='batch',\n",
    "                                random_state=0)\n",
    "start_time = time.time()\n",
    "lda.fit(tf)\n",
    "\n",
    "print ('Fit phase of LDA took {0:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prints given number of top words for a LDA model\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    matr = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        \n",
    "        message += \" \".join([str(feature_names[i]) + \": \" + \"{:.5f}\".format(matr[topic_idx, i])\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List topics, retrieved by LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: euro: 0.03303 milliarde: 0.01997 deutlich: 0.00992 prozent: 0.00923 ergebnis: 0.00920 hoch: 0.00916 vorjahr: 0.00906 million: 0.00784 liegen: 0.00776 positiv: 0.00718\n",
      "\n",
      "Topic #1: mio: 0.04605 mrd: 0.03011 ertrag: 0.01223 höhe: 0.01143 aktie: 0.00991 quartal: 0.00960 dezember: 0.00955 betragen: 0.00945 aufwendung: 0.00909 anstieg: 0.00748\n",
      "\n",
      "Topic #2: risiko: 0.02038 konzern: 0.00869 intern: 0.00769 wesentlich: 0.00667 rahmen: 0.00654 basis: 0.00587 steuerung: 0.00556 kapital: 0.00522 risk: 0.00511 erfolgen: 0.00485\n",
      "\n",
      "Topic #3: bank: 0.01819 kunde: 0.00927 segment: 0.00715 bereich: 0.00683 mein: 0.00618 deutschen: 0.00533 mitarbeiter: 0.00526 konzern: 0.00510 management: 0.00465 unternehmen: 0.00407\n",
      "\n",
      "Topic #4: milliarde: 0.01189 kunde: 0.00722 geschäft: 0.00619 markt: 0.00595 position: 0.00525 transaktion: 0.00503 dezember: 0.00491 segment: 0.00486 helaba: 0.00481 finance: 0.00466\n",
      "\n",
      "Topic #5: million: 0.07593 euro: 0.02452 höhe: 0.01731 eur: 0.01609 vorjahr: 0.01517 verlust: 0.00995 dezember: 0.00740 betragen: 0.00683 bank: 0.00650 risiko: 0.00621\n",
      "\n",
      "Topic #6: fair: 0.01376 value: 0.01205 beizulegenden: 0.01195 vermögenswerte: 0.01180 verbindlichkeit: 0.00987 bewerten: 0.00965 zeitwert: 0.00914 konzern: 0.00827 finanziell: 0.00813 ausweisen: 0.00770\n",
      "\n",
      "Topic #7: ifrs: 0.01868 unternehmen: 0.01433 konzern: 0.01427 bank: 0.01419 ias: 0.00842 standard: 0.00789 änderung: 0.00664 konzernabschluss: 0.00647 deutschen: 0.00597 auswirkung: 0.00564\n",
      "\n",
      "Topic #8: aufsichtsrat: 0.01305 mitglied: 0.01018 vorstehen: 0.00933 vorstand: 0.00819 aufsichtsrats: 0.00754 bank: 0.00716 nordlb: 0.00698 dezember: 0.00632 tabelle: 0.00598 vergütung: 0.00568\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic #2 resembles \"Risk Management\". Let us collect information about this topic in annual report of CommerzBank for the year 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMMERZBANK_FILE = 'Commerzbank-AnnualReport-2016.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commerz_paragraphs, names = get_clean_data([COMMERZBANK_FILE],get_paragraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commerz_paragraphs_numbers = readPageAndParInfo(FILE_PATH + COMMERZBANK_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform phase of LDA took 0.21 seconds\n"
     ]
    }
   ],
   "source": [
    "tf_commerz = tf_vectorizer.transform(commerz_paragraphs[0])\n",
    "\n",
    "start_time = time.time()\n",
    "topic_model = lda.transform(tf_commerz)\n",
    "\n",
    "print ('Transform phase of LDA took {0:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect information about risk management for all paragraphs of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank_risk_management = []\n",
    "for doc, document_name in enumerate(range(len(commerz_paragraphs[0]))):\n",
    "    company_name = document_name\n",
    " \n",
    "    #print('\\n{:40.40}: '.format(str(document_name)), end ='')\n",
    "    most_probable = np.argsort(topic_model[doc, :])[:-6:-1]\n",
    "\n",
    "    cummulated = 0\n",
    "    for topic in most_probable:\n",
    "\n",
    "        probability = topic_model[doc, topic]\n",
    "        if int(topic) == 2:\n",
    "            bank_risk_management.append({\n",
    "                'paragraph': company_name,\n",
    "                'value': str(probability)\n",
    "            })\n",
    "        # print('{:6.2%} {:3} '.format(probability, topic), end = '')\n",
    "        cummulated = cummulated + probability\n",
    "        if cummulated > 0.95: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add extra information about page, which contain paragraph of interest in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_risk_management = sorted(bank_risk_management, key=lambda k: -1 * float(k['value']))\n",
    "top_result = [paragraph for paragraph in sorted_risk_management if float(paragraph['value']) > 0.5]\n",
    "\n",
    "value_sum = 0.0\n",
    "top_result_page_number = []\n",
    "for result in top_result:\n",
    "    item = {}\n",
    "    value_sum += float(result['value'])\n",
    "    item['paragraph'] = result['paragraph']\n",
    "    item['value'] = result['value']\n",
    "    item['page'] = commerz_paragraphs_numbers[result['paragraph']]['page']\n",
    "    item['page_par'] = commerz_paragraphs_numbers[result['paragraph']]['paragraph']\n",
    "    top_result_page_number.append(item)\n",
    "top_result_page_number = sorted(top_result_page_number, key=lambda k:int(k['page']) * 100 + int(k['page_par']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group paragraphs by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_topic_dict = dict()\n",
    "for k, v in groupby(top_result_page_number, lambda x: x['page']):\n",
    "    page_topic_dict[k] = list(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign maximum value of the paragraph on the page to the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "pages = []\n",
    "texts = []\n",
    "for k,v in page_topic_dict.items():\n",
    "    value = max([item['value'] for item in v])\n",
    "    values.append(value)\n",
    "    pages.append(k)\n",
    "    texts.append('Page: ' + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot topic distribution throughout pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"1641320b-5b22-43fb-9ac0-45450a170da2\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "        Plotly.plot(\n",
       "            '1641320b-5b22-43fb-9ac0-45450a170da2',\n",
       "            [{\"text\": [\"Page: 1\", \"Page: 43\", \"Page: 44\", \"Page: 45\", \"Page: 52\", \"Page: 53\", \"Page: 61\", \"Page: 62\", \"Page: 85\", \"Page: 95\", \"Page: 96\", \"Page: 97\", \"Page: 98\", \"Page: 99\", \"Page: 101\", \"Page: 102\", \"Page: 104\", \"Page: 105\", \"Page: 113\", \"Page: 116\", \"Page: 117\", \"Page: 118\", \"Page: 119\", \"Page: 120\", \"Page: 122\", \"Page: 123\", \"Page: 124\", \"Page: 125\", \"Page: 170\", \"Page: 247\", \"Page: 248\", \"Page: 249\", \"Page: 250\", \"Page: 253\", \"Page: 299\", \"Page: 300\", \"Page: 301\", \"Page: 303\", \"Page: 304\", \"Page: 305\", \"Page: 311\", \"Page: 312\", \"Page: 313\", \"Page: 314\", \"Page: 319\", \"Page: 322\"], \"x\": [1, 43, 44, 45, 52, 53, 61, 62, 85, 95, 96, 97, 98, 99, 101, 102, 104, 105, 113, 116, 117, 118, 119, 120, 122, 123, 124, 125, 170, 247, 248, 249, 250, 253, 299, 300, 301, 303, 304, 305, 311, 312, 313, 314, 319, 322], \"y\": [\"0.8400378613017087\", \"0.9441950012532824\", \"0.9545248112575585\", \"0.9429047152523939\", \"0.8271040944620622\", \"0.9191535141929615\", \"0.6811454777063438\", \"0.5358966431328503\", \"0.7056039314205456\", \"0.9693357787878143\", \"0.8464917018250733\", \"0.9844530004394574\", \"0.9876493745751717\", \"0.8876113554453039\", \"0.5942301220200683\", \"0.9150061618181254\", \"0.8725314885135069\", \"0.6528459238865187\", \"0.8282675540569023\", \"0.8532141098887628\", \"0.7935590698360939\", \"0.6769392398806209\", \"0.9814744133815241\", \"0.8144160016789147\", \"0.6326870894772714\", \"0.9558618270862236\", \"0.9254208793818328\", \"0.9713093738960803\", \"0.6214448398714933\", \"0.6998824410829775\", \"0.8248441367416471\", \"0.9315986563031791\", \"0.9490000882515368\", \"0.6454919270697533\", \"0.8262085264265273\", \"0.6153209119558772\", \"0.5787482592899007\", \"0.9835322476091671\", \"0.9752990773006613\", \"0.9832181536812755\", \"0.8117537429319994\", \"0.9477061148004117\", \"0.882979087414922\", \"0.9693404691128722\", \"0.5828203593429617\", \"0.8400378613017087\"], \"type\": \"bar\", \"uid\": \"1cb45d52-a3c9-11e8-8a3b-b14a76472061\"}],\n",
       "            {\"title\": \"Commerzbank 2016 Report\", \"xaxis\": {\"title\": \"Pages\"}, \"yaxis\": {\"title\": \"Probability\"}},\n",
       "            {\"showLink\": true, \"linkText\": \"Export to plot.ly\"}\n",
       "        ).then(function () {return Plotly.addFrames('1641320b-5b22-43fb-9ac0-45450a170da2',{});}).then(function(){Plotly.animate('1641320b-5b22-43fb-9ac0-45450a170da2');})\n",
       "        });</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = go.Bar(\n",
    "    x=pages,\n",
    "    y=values,\n",
    "    text=texts\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "    title='Commerzbank 2016 Report',\n",
    "    xaxis=dict(\n",
    "        title='Pages'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Probability'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig, filename='text-hover-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, after using LDA, the person who has to analyze document can now find pages, containing topic of interest without reading the whole document and even if this topic is not stated in the table of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us now move to the second use case [2]: Topic fluctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat experiment, but instead of paragraphs of one document, take complete documents of different banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "banks_documents, names = get_clean_data(banks,get_paragraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_banks = tf_vectorizer.transform(banks_documents)\n",
    "topic_model_banks = lda.transform(tf_banks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print()\n",
    "#print(' Dominant topics per document ')\n",
    "#print('------------------------------')\n",
    "\n",
    "bank_risks = {}\n",
    "for doc, document_name in enumerate([file for file in banks]):\n",
    "    if 'Annual' not in document_name:\n",
    "        continue\n",
    "    company_name = document_name[:document_name.find('-')]\n",
    "    \n",
    "    if company_name not in bank_risks:\n",
    "        bank_risks[company_name] = []\n",
    "        \n",
    "    \n",
    "    #print('\\n{:40.40}: '.format(document_name), end ='')\n",
    "    most_probable = np.argsort(topic_model_banks[doc, :])[:-6:-1]\n",
    "\n",
    "    cummulated = 0\n",
    "    for topic in most_probable:\n",
    "\n",
    "        probability = topic_model_banks[doc, topic]\n",
    "        if int(topic) == 2:\n",
    "            year = document_name[document_name.rfind('-') + 1:document_name.rfind('-') + 5]\n",
    "            bank_risks[company_name].append({\n",
    "                'year': year,\n",
    "                'value': str(probability)\n",
    "            })\n",
    "        #print('{:6.2%} {:3} '.format(probability, topic), end = '')\n",
    "        cummulated = cummulated + probability\n",
    "        if cummulated > 0.95: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'year': '2015', 'value': '0.16415967578501264'},\n",
       " {'year': '2013', 'value': '0.19845716278419687'},\n",
       " {'year': '2010', 'value': '0.11417129827446894'},\n",
       " {'year': '2014', 'value': '0.20251953613237136'},\n",
       " {'year': '2016', 'value': '0.16164756455642815'},\n",
       " {'year': '2012', 'value': '0.17401214059577957'},\n",
       " {'year': '2011', 'value': '0.12702646388376182'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_risks['DeutscheBank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot topic fluctuation throughout years for different banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayerischeLandesbank\n",
      "[25.99, 25.18, 28.49, 17.39, 17.33, 17.89]\n",
      "DeutscheBank\n",
      "[12.7, 17.4, 19.85, 20.25, 16.42, 16.16]\n",
      "DzBank\n",
      "[17.09, 16.62, 18.14, 19.71, 22.04, 21.79]\n",
      "Commerzbank\n",
      "[9.7, 11.52, 11.88, 13.47, 13.13, 15.41]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"2bb142a7-1d82-4b1f-a73b-13a2735f46e9\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2bb142a7-1d82-4b1f-a73b-13a2735f46e9\", [{\"mode\": \"lines+markers\", \"name\": \"BayerischeLandesbank\", \"x\": [2011, 2012, 2013, 2014, 2015, 2016], \"y\": [25.99, 25.18, 28.49, 17.39, 17.33, 17.89], \"type\": \"scatter\", \"uid\": \"8adee90a-a3c9-11e8-8a3b-b14a76472061\"}, {\"mode\": \"lines+markers\", \"name\": \"DeutscheBank\", \"x\": [2011, 2012, 2013, 2014, 2015, 2016], \"y\": [12.7, 17.4, 19.85, 20.25, 16.42, 16.16], \"type\": \"scatter\", \"uid\": \"8adeeb12-a3c9-11e8-8a3b-b14a76472061\"}, {\"mode\": \"lines+markers\", \"name\": \"DzBank\", \"x\": [2011, 2012, 2013, 2014, 2015, 2016], \"y\": [17.09, 16.62, 18.14, 19.71, 22.04, 21.79], \"type\": \"scatter\", \"uid\": \"8adeec48-a3c9-11e8-8a3b-b14a76472061\"}, {\"mode\": \"lines+markers\", \"name\": \"Commerzbank\", \"x\": [2011, 2012, 2013, 2014, 2015, 2016], \"y\": [9.7, 11.52, 11.88, 13.47, 13.13, 15.41], \"type\": \"scatter\", \"uid\": \"8adeed60-a3c9-11e8-8a3b-b14a76472061\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [2011,2012, 2013, 2014, 2015, 2016]\n",
    "values = []\n",
    "traces = []\n",
    "bank_of_interest = ['DeutscheBank', 'Commerzbank', 'BayerischeLandesbank', 'DzBank']\n",
    "for key, value in bank_risks.items():\n",
    "    if key not in bank_of_interest:\n",
    "        continue\n",
    "    newlist = sorted(bank_risks[key], key=lambda k: int(k['year']))\n",
    "    for l in newlist:\n",
    "        values.append(round(float(l['value']) * 100,2))\n",
    "    if key != 'Commerzbank':\n",
    "        values = values[1:]\n",
    "    print (key)    \n",
    "    print (values)\n",
    "    trace = go.Scatter(\n",
    "        x = years,\n",
    "        y = values,\n",
    "        mode = 'lines+markers',\n",
    "        name = key\n",
    "    )\n",
    "    traces.append(trace)\n",
    "    values = []\n",
    "py.offline.iplot(traces, filename='scatter-mode.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the percentage of the topic of \"Risk Management\" fluctuated in annual reports of different banks throughout several years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "- LDA is quite complicated, but allows meaningful topic detection. \n",
    "- Topic labeling should be done manually, but only once per set of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
